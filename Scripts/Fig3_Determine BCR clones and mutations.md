## 1. Analysis using Change-O toolkit

### Converting 10X V(D)J data into the AIRR Community standardized format

```bash
cd ./10x_data
AssignGenes.py igblast -s filtered_contig.fasta -b igblast_1.17.1 \
--organism human --loci ig --format blast
#The -b argument specifies the path containing the database, internal_data, and optional_file directories required by IgBLAST.
# The output is "filtered_contig_igblast.fmt7".
MakeDb.py igblast -i filtered_contig_igblast.fmt7 -s filtered_contig.fasta -r \
imgt_human_*.fasta \
--10x filtered_contig_annotations.csv --extended
# The output is "filtered_contig_igblast_db-pass.tsv", which overwrites the V, D and J gene assignments generated by Cell Ranger and uses those generated by IgBLAST instead.
```

### Identifying clones from B cells in AIRR formatted 10X V(D)J data

1. Splitting into separate light and heavy chain files. To group B cells into clones from AIRR Rearrangement data, the output from MakeDb must be parsed into a light chain file and a heavy chain file:

```bash
ParseDb.py select -d filtered_contig_igblast_db-pass.tsv -f locus -u "IGH" \
        --logic all --regex --outname temp
ParseDb.py select -d temp_parse-select.tsv -f productive -u T --outname temp2
ParseDb.py select -d temp2_parse-select.tsv -f v_call j_call c_call -u "IGH" \
    --logic all --regex --outname heavy

ParseDb.py select -d filtered_contig_igblast_db-pass.tsv -f locus -u "IG[LK]" \
        --logic all --regex --outname temp
ParseDb.py select -d temp_parse-select.tsv -f productive -u T --outname temp2
ParseDb.py select -d temp2_parse-select.tsv -f v_call j_call c_call -u "IG[LK]" \
    --logic all --regex --outname light
# the outputs are "heavy_parse-select.tsv" and "light_parse-select.tsv". Non-productive sequences were removed.
#records with disagreements between the C-region primers and the reference alignment were removed too 
#(vjc are both IGH or IGL/LGK).
```

2. Calculating nearest neighbor distances based on heavy chains

```bash
library(shazam);library(ggplot2)
data(ExampleDb, package="alakazam")
heavy_parse <- read.table("./heavy_parse-select.tsv",sep = "\t",header = T,stringsAsFactors = F)
dist_ham <- distToNearest(heavy_parse, sequenceColumn="junction", 
                          vCallColumn="v_call", jCallColumn="j_call",
                          model="ham", normalize="len", nproc=1)
output_ham <- findThreshold(dist_ham$dist_nearest, method="density")
dist_s5f <- distToNearest(ExampleDb, sequenceColumn="junction", 
                          vCallColumn="v_call", jCallColumn="j_call",
                          model="hh_s5f", normalize="none", nproc=1)
output_s5f <- findThreshold(dist_s5f$dist_nearest, method="density")
output_ham@threshold;output_s5f@threshold# show the threshold
```

3. Clonal assignment using shazam

```bash
DefineClones.py -d heavy_parse-select.tsv --act set --model ham \
    --norm len --dist 0.16
# or use other models:
#DefineClones.py -d heavy_parse-select.tsv --act set --model hh_s5f --norm none --dist **
# output is "heavy_parse-select_clone-pass.tsv"

#Correct clonal groups based on light chain data:
light_cluster.py -d heavy_parse-select_clone-pass.tsv -e light_parse-select.tsv \
        -o 10X_clone-pass.tsv
#The algorithm will (1) remove cells associated with more than one heavy chain and 
#(2) correct heavy chain clone definitions based on an analysis of the light chain partners
#associated with the heavy chain clone.
```

### Reconstructing germline sequences

```bash
CreateGermlines.py -d 10X_clone-pass.tsv -g dmask --cloned\
    -r $SCRATCH/projects/B/immcantation/germlines/imgt/human/vdj/imgt_human_*.fasta
# The output is "10X_clone-pass_germ-pass.tsv".
#this will generate a single germline of consensus length for each clone
```



## 2. Define SHM (mutation analysis)

```bash
https://shazam.readthedocs.io/en/stable/vignettes/Mutation-Vignette/
```

## 3. Clonal abundance and diversity

```bash
https://alakazam.readthedocs.io/en/stable/vignettes/Diversity-Vignette/
```
